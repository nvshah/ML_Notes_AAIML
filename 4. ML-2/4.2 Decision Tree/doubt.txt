doubt

Q) Why Entropy possess log() ?
 -> Information Theory

    https://stats.stackexchange.com/questions/87182/what-is-the-role-of-the-logarithm-in-shannons-entropy

    https://www.quora.com/Why-is-there-a-logarithmic-function-in-the-entropy-formula
    https://bricaud.github.io/personal-blog/entropy-in-decision-trees/

    1. It is practically more useful
    2. It is nearer to our intutive feeling as to the proper measure
    3. It is mathematically more sutiable

Q) How Train Time Complexity of DT is O(n*lg(n)*d) ??


Q) How partial derivative of Summation(Px) = 1  ?? (in proof of Entropy & Uniform Distb)

