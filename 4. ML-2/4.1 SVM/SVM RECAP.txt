SVM RECAP


libsvm is one of the best library for training SVM


REMEMBER :-
You can have your Hyper-Param Either multiplied to loss or multiplied to Regularizer
Both of them works & allowed

If you dont know best kernel - Use RBF-Kernel (2 HyperParam : C, SIGMA)
C := For Loss Func (Soft-Margin)
SIGMA := Controls Distance in RBF Kernel

NOTE :
  -> In your algo you dont have way to control your #Support-Vectors in traditional standard

_____________________________

[Geometrically]
Key Idea :- Find Hyper-plane that seperates points as widely as posible (ie Margin Maximizing HyperPlane)

[MINE UNDERSTANDING DEDUCING] <--**
Q) Why SVM focus on finding Margin Maximizer Hyper-Plane ??

 -> Because We have seen in Logistic Regression, that the closer the point to
    Seperating Hyper-Plane, the lesser confidant we are about its belonging
    |
    So SVM take learning from the Logistic Regression & Tries to mitigate that
    conundrum via finding such seprating plane that has possibly maximum Margin

=> Such plane is called as "Margin-Maximizing Hyper-plane"

NOTE :
 The margin on both the side of plane must be symmetric ie
 Equal distance of boundaries on both the side
 &
 Hence only we can state total distance between 2 boundaries = 2 / ||w||
 

* Hinge Loss :
  -------
  -> As the Hinge Loss itself is not differentiable at point 1
     So there are optimization Hack to achieve this !

* 1 - Zi -> Zeta

  For correct -> Zeta is 0
  For Incorrect -> Zeta is 1-Zj

  Zeta is the distance of point from correct boundary (ie Support Vectors)

  & This can be represented by max(0, 1-Zi) := Zetai

 In Soft-SVM :-

 [CONSTRAINT-1]
  Now when we say 1-Zi >= Zetai in constraints, we mean to say that
  those pts who are far away in -ve direction (ie misclassified)

  Note Zetai is > 0 for Incorrectly classified pts
  Now
  Our Prediction would be Zi & we use (1-Zi)
  Thus
  1-Zi is the Loss we face for our prediction & that what is the constraints

 [CONSTRAINT-2]
  Zeta > 0  (ie Positive)

* Soft SVM vs Loss-Minimization :
  -------
  REMEMBER :-
    You can have your Hyper-Param Either multiplied to loss or multiplied to Regularizer
    Both of them works & allowed

  -> Major difference is of Hyper-Param
     In Soft SVM := C is used as Hyper-Param with Loss term
     In Loss-Minima := LMA is used as Hyper-Param with Reg term


* Primal Form of SVM :
  ----
  Soft SVM or Loss Minima (ie Hinge Loss)

  Given query Point Xq,
  I want to find the (W*Xq + b) = f(Xq) := Yq


* Dual Form of SVM :
  ----
  In Optimization Theory this primal form can be represented in different
  formulation

  1) For every Xi -> ALPHAi
  2) Xi only occurs in Xi*Xj

[REMEMBER]
Now In both form we aim to find the Yq, given Xq

  3) F(Xq) := Sum(ALPHAi*yi*(Xi*Xq)) + b
  4) ALPHAi is > 0 for only Support Vectors

  => To compute/predict for Xq, the only point matters are Support Vectros
     Hence Algo named Support Vectors Machines

     Non-Support vectors do not change the alog how behaves

  => If someone provides you similarity matrix
     then in dual form it makes sense
     As
     We have some formulation in terms of 2 points in equation

  => Dual Form accounts the reln betn the points in terms of Similarity

REMEMBER :
The main reason we get into primal to dual form is because we can
leverage the Kernalization (ie Similarity Manipulation) whilst solving the
optimization Problem

=> Similarity is the pivotal thing for Dual Form transition

=> Even at RunTime we can substitute our Xi_T*Xj into K(Xi, Xj)

* Soft SVM Idea & Logistic Reg :
  -----------
  Soft SVM idea is very similar to Logistic Reg, except that it is Margin Maximization based concept

* Similarity Concept :
  -----
  - helps us to deal with formulation at Run-Time as well very easily
  - If Simple Dot product is used then its similar to Linear SVM !

* Linear SVM :
  -> In Linear SVM you try to find Margin-Maximization in a Space of your Xi's
  Similarly
     In Logistic Regression you try to find Loss in space of your Xi's

     (Hence most often Linear SVM & Logistic Reg provides close results)

  => Margin Maximization is good idea but its not world changing Idea
     The world changing Idea is Kernelization (ie Changing Space for your model)

     Kernel SVM := Changing Space := Derive new Features -> Feature Transformation

* Kernel SVM :
  ----
  Kernelization helps SVM to deal with Non-Linea Seperable Points

  - Kernalization does Implicit Feature Transform for us
  - Question is abt finding right kernels

  Hence Explicit Feature Transform in SVM is replaced by Finding Right Kernel


* RBF-Kernel :
  ----
  - Accounts distance between 2 Points
  - As Distance Increases K-val (ie Kernel Similarity) Falls Exponentially
    Hence as point moves farther away it makes simiarity closer to 0
    (ie Distance tends to Infinity)

  - HyperParam :- SIGMA
      |
      -> SIGMA helps to narrow/expand the Viccinity Criteria of Neighbors in RBF-Kernel
         Hence
         deciding the Similarity/DisSimilarity in case of RBF-Kernel
         It's Analogous to K in KNN

Kernel are like Similarity Func
Distance are like DisSimilarity Functions

REMEMBER : MIMP Task in ML is Featurization

* There are other special types of Kernel (ie Domain Specific)
  ---
  Feature Transform + Domain Knowledge := Domain Specific Kernel
  Eg
  1. String Kernels
  2. Genome Kernels


* SVM vs Logistic :
  ----------
  In case of Logistic Reg, to deal with little bit complex algo
  We use transformation such as log(), x^2, x^3, ...

  Now In case of SVM, we use diff kernels that share same purpose as above transformation
  But in this case there are domain-specific as well !


* Complexities :

  Run Time Complexity for SVM depends on #Support-Vectors

NOTE :
  -> In your algo you dont have way to control your #Support-Vectors in traditional standard SVM


* SVR (Support Vector Regression)
  --------------
  -> In case of SVR, EPSILON is hyper Param
     &
     Main Aim :- the absolute diff between actual & pred should be less than EPSILON (ie HyperParam)

  -> RBF SVR works actually like K-NN

----------------------------------
PRACT

There are specialized algo such as SMO-SVM


---------------

* CASES

- Kernel Tricks takes point Xi (d-dimen) into another dimension d` & thus give another point Xi`
- Impact of Outliers is minimal as SVM relies on Support Vectors more than exact point whilst prediction
- large dimension point are good for SVM
