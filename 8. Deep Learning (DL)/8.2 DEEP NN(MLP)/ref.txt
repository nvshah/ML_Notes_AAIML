ref

[Neural Network]
https://en.wikipedia.org/wiki/Rectifier_(neural_networks)

- Saddle Point (https://en.wikipedia.org/wiki/Saddle_point )

- Grad + Momentum 
     - https://www.slideshare.net/SebastianRuder/optimization-for-deep-learning
     - https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d

     - refer sebastian Ruder Blog (Optimizing GGradient Descent)
     - https://ruder.io/optimizing-gradient-descent

- Types of Optimization Algo (Optimizers)
   - https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f
   - https://www.kdnuggets.com/2020/12/optimization-algorithms-neural-networks.html

- Auto Encoders 
	http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/
	https://en.wikipedia.org/wiki/Autoencoder

- Word2Vec (**IMP**)
    https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/


----------------
Books

[ImageNet Classif]
http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf

[Batch Normalization]
https://arxiv.org/pdf/1502.03167v3.pdf

[ADAM PDF Book paper Published]
https://arxiv.org/pdf/1412.6980.pdf

-----------------
Videos

[Adagrad- Dense & Sparse Features]
https://www.youtube.com/watch?v=c86mqhdmfL0
