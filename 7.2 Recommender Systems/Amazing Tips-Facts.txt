Amazing Tips-Facts

* Frobenius Norm :
  -------
   X - Matrix :- ||X||^2    // sum each element of matrix after squaring it


* NMF & Elastic net :
  --------
  
  NMF objective func in sklearn uses Elastic net 

  Objective func :- X = W*H

  If you want W & H to be sparse Matrix (more) consider providing more weight to l1-regularization multiplication term ie l1_ratio

  so if you provide more weight to L1-Regularization in Elastic Net it will provide more sparsity compare to L2-Regularization

=> L1 regularizations creates sparsity


* MF as Optimization Problem :
  -----
   - Advantage :- able to account other system biases easily

* SVD & Featurization :
  ------
  SVD helps to get lower-dimen features for a Point
  Now
  via SVD we get 2 Matrix i.e Left Singular Matrix & Right Singular Matrix
  We can use both of these matrix constituting 2*k dimen in total for single data point

  k-features from Left Singular Matrix
  k-features from Right Singular Matrix

  Hence 2*k features all in all for Single Data Point


* SVD & Adj-Matrix :
  ----
  -> Adj-Matrix is helpful to derive SVD
  -> For single row(item) in Adj-Matrix, We have reln matrix for all other items as a column
     &
     thus let say if we have c-columns

     Then with the help of SVD we can reduce it to 2*k where k is dimen to reduce

  -> Note we will get 2 vectors representation for Single Row Item
